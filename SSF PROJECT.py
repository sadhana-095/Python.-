# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/167qD9sYm4rBGQwhx-a2OS6Bqk-OenYCw
"""

from pyspark.sql import SparkSession
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline

# Create Spark session
spark = SparkSession.builder.appName("BankStatementML").getOrCreate()

# Load your uploaded CSV (make sure path is correct in Colab)
df = spark.read.csv("/content/simulated_bank_statement.csv", header=True, inferSchema=True)

# Index the target column (Category ‚Üí numeric label)
indexer = StringIndexer(inputCol="Category", outputCol="label")

# (Optional) Use Amount as numeric feature
assembler = VectorAssembler(inputCols=["Amount"], outputCol="features")

# Classifier
lr = LogisticRegression(maxIter=20, regParam=0.01)

# Build pipeline
pipeline = Pipeline(stages=[indexer, assembler, lr])

# Train/test split
train, test = df.randomSplit([0.8, 0.2], seed=42)

# Train model
model = pipeline.fit(train)

# Predict
predictions = model.transform(test)
predictions.select("Date", "Category", "Amount", "prediction").show(20, truncate=False)



from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as _sum

# Create Spark session
spark = SparkSession.builder.appName("BankStatementGroupedSpending").getOrCreate()

# Load CSV (adjust path for Colab/local)
df = spark.read.csv("/content/simulated_bank_statement.csv", header=True, inferSchema=True)

# Group by category and sum the amounts
grouped = df.groupBy("Category").agg(_sum("Amount").alias("Total_Spending"))

# Show results
grouped.show(truncate=False)

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as spark_sum

# 1. Start Spark session
spark = SparkSession.builder.appName("PersonalFinanceCategorizer").getOrCreate()

# 2. Load your dataset
file_path = "simulated_bank_statement.csv"  # change path if needed
df = spark.read.csv(file_path, header=True, inferSchema=True)

# 3. Group spending by category
grouped = df.groupBy("Category").agg(spark_sum("Amount").alias("Total_Spending"))

# 4. Show result
grouped.show(truncate=False)

# 5. Save result to CSV (optional)
grouped.write.csv("grouped_spending_output", header=True)

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as spark_sum
from tabulate import tabulate
from termcolor import colored

# 1. Start Spark session
spark = SparkSession.builder.appName("PersonalFinanceCategorizer").getOrCreate()

# 2. Load dataset
file_path = "simulated_bank_statement.csv"  # make sure path is correct
df = spark.read.csv(file_path, header=True, inferSchema=True)

# 3. Group spending by category
grouped = df.groupBy("Category").agg(spark_sum("Amount").alias("Total_Spending"))

# 4. Convert Spark DataFrame ‚Üí Pandas for styling
pandas_df = grouped.toPandas()

# 5. Format "Total_Spending" with bold + cyan color
pandas_df["Total_Spending"] = pandas_df["Total_Spending"].apply(
    lambda x: colored(f"{x:,.2f}", "cyan", attrs=["bold"])
)

# 6. Print colored table
print("\n" + colored("üìä Spending Summary by Category", "yellow", attrs=["bold", "underline"]))
print(tabulate(pandas_df, headers="keys", tablefmt="fancy_grid", showindex=False))

# 7. Save result to CSV (overwrite if exists)
grouped.write.mode("overwrite").csv("grouped_spending_output", header=True)

!apt-get install openjdk-11-jdk -y

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["PATH"] += ":/usr/lib/jvm/java-11-openjdk-amd64/bin"

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ColabSparkUI") \
    .config("spark.ui.port", "4040") \
    .getOrCreate()

print("‚úÖ Spark session created successfully!")

!pip install pyngrok
from pyngrok import ngrok

# Replace YOUR_AUTHTOKEN_HERE with your copied token
ngrok.set_auth_token("34xgcbA1yQFj4ZEGoa3DaSk58pY_aSdhMREEeQ6MNrLZSszU")
spark_ui_tunnel = ngrok.connect(4040)
print("Spark Web UI link:", spark_ui_tunnel.public_url)

!pip install pyspark tabulate termcolor matplotlib seaborn

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("PersonalFinanceCategorizer") \
    .config("spark.ui.port", "4040") \
    .getOrCreate()

!echo "Open Spark UI at: http://localhost:4040"

!pip install pyspark tabulate termcolor flask-ngrok pyngrok matplotlib seaborn plotly

from pyspark.sql import SparkSession

spark = (SparkSession.builder
         .appName("PersonalFinanceCategorizer")
         .config("spark.ui.port", "4040")
         .getOrCreate())

print("‚úÖ Spark session started")

from pyngrok import ngrok
spark_ui = ngrok.connect(4040, "http")
print("üåê Spark UI:", spark_ui.public_url)

from pyspark.sql.functions import sum as spark_sum
from termcolor import colored
from tabulate import tabulate

file_path = "simulated_bank_statement.csv"   # adjust if needed
df = spark.read.csv(file_path, header=True, inferSchema=True)

grouped = df.groupBy("Category").agg(spark_sum("Amount").alias("Total_Spending"))
pandas_df = grouped.toPandas()

# Pretty print in terminal
pandas_df["Total_Spending"] = pandas_df["Total_Spending"].apply(
    lambda x: colored(f"{x:,.2f}", "cyan", attrs=["bold"])
)
print("\n" + colored("üìä Spending Summary by Category", "yellow", attrs=["bold", "underline"]))
print(tabulate(pandas_df, headers="keys", tablefmt="fancy_grid", showindex=False))

# Save aggregated CSV
grouped.write.mode("overwrite").csv("/content/grouped_spending_output", header=True)

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

pdf = grouped.toPandas()

# Matplotlib / Seaborn
plt.figure(figsize=(8,5))
sns.barplot(x="Category", y="Total_Spending", data=pdf, palette="cool")
plt.title("üí∞ Spending Summary by Category", fontsize=14, fontweight="bold")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Interactive Plotly version
fig = px.pie(pdf, values="Total_Spending", names="Category",
             title="Spending Breakdown by Category", hole=0.4,
             color_discrete_sequence=px.colors.sequential.Viridis)
fig.show()

from flask import Flask
app = Flask(__name__)

@app.route("/")
def index():
    return fig.to_html(full_html=True)

flask_ui = ngrok.connect(5000, "http")
print("üß≠ Finance Chart Dashboard:", flask_ui.public_url)

app.run(port=5000)

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
        .master("local[*]")
        .appName("ColabSparkUI")
        .config("spark.ui.port", "4040")   # Spark UI port
        .getOrCreate()
)

!pip install pyngrok
from pyngrok import ngrok

spark_ui_tunnel = ngrok.connect(4040)
print("üîó Spark Web UI:", spark_ui_tunnel.public_url)